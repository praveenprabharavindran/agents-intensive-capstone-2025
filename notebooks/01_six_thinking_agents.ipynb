{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a7786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 1. Load environment variables from the .env file\n",
    "# By default, this looks for a .env file in the current directory\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Access the variable\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"âœ… Gemini API key setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.adk.agents import Agent, ParallelAgent, SequentialAgent\n",
    "from google.adk.models.google_llm import Gemini\n",
    "\n",
    "#Define the Model (Shared or unique per agent)\n",
    "model = Gemini(model_name=\"gemini-2.5-flash-lite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71464712",
   "metadata": {},
   "source": [
    "### LLM lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c300734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, litellm \n",
    "from google.adk.models.lite_llm import LiteLlm \n",
    "os.environ[ 'LITELLM_PROXY_API_KEY' ] = os.environ[ 'OPENAI_API_KEY' ]\n",
    "os.environ[ 'LITELLM_PROXY_API_BASE' ] = os.environ[ 'OPENAI_API_BASE' ] \n",
    "litellm.use_litellm_proxy = True \n",
    "litellm_model = LiteLlm( model=\"gpt-oss-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafffed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.models.lite_llm import LiteLlm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3785d1",
   "metadata": {},
   "source": [
    "### Simple agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe3581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.tools import google_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02728597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def get_current_time() -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dictionary with the current time in the format YYYY-MM-DD HH:MM:SS.\n",
    "    \"\"\"\n",
    "    return {\"current_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8894f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_agent = Agent(\n",
    "    name=\"simple_agent\",\n",
    "    model=litellm_model,\n",
    "    instruction=\"You are a helpful agent. Usee the following tools to answer the user question: - get_current_time\",\n",
    "    output_key=\"simple_agent_response\",\n",
    "    tools=[ get_current_time],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c8e22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.plugins.logging_plugin import (\n",
    "    LoggingPlugin,\n",
    ")  # <---- 1. Import the Plugin\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "\n",
    "runner = InMemoryRunner(\n",
    "    agent=root_agent,\n",
    "    plugins=[\n",
    "        LoggingPlugin()\n",
    "    ],  # <---- 2. Add the plugin. Handles standard Observability logging across ALL agents\n",
    ")\n",
    "print(\"âœ… Runner configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f03a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Running agent with LoggingPlugin...\")\n",
    "print(\"ðŸ“Š Watch the comprehensive logging output below:\\n\")\n",
    "\n",
    "response = await runner.run_debug(\"what is the new on telsa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e78e5fa",
   "metadata": {},
   "source": [
    "## Six thinking hats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e1b18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 5 \"Thinking\" Agents (The Team)\n",
    "# WHITE HAT: Facts & Data\n",
    "white_hat = Agent(\n",
    "    name=\"WhiteHat\",\n",
    "    model=model,\n",
    "    instruction=\"You are the White Hat. Focus ONLY on available data, facts, and information gaps. Be objective and neutral. Do not offer opinions, only verifiable facts or questions about missing data.\",\n",
    ")\n",
    "\n",
    "# RED HAT: Emotions & Intuition\n",
    "red_hat = Agent(\n",
    "    name=\"RedHat\",\n",
    "    model=model,\n",
    "    instruction=\"You are the Red Hat. Focus on intuition, hunches, and emotional reaction. How does this problem make users or the team feel? You do not need to justify your feelings with logic.\",\n",
    ")\n",
    "\n",
    "# BLACK HAT: Caution & Risk\n",
    "black_hat = Agent(\n",
    "    name=\"BlackHat\",\n",
    "    model=model,\n",
    "    instruction=\"You are the Black Hat. Play the devil's advocate. Identify specific risks, potential failure points, downsides, and why this idea might NOT work. Be critical.\",\n",
    ")\n",
    "\n",
    "# YELLOW HAT: Optimism & Benefits\n",
    "yellow_hat = Agent(\n",
    "    name=\"YellowHat\",\n",
    "    model=model,\n",
    "    instruction=\"You are the Yellow Hat. Focus on the positives. What are the benefits? What is the best-case scenario? Why will this work?\",\n",
    ")\n",
    "\n",
    "# GREEN HAT: Creativity & Alternatives\n",
    "green_hat = Agent(\n",
    "    name=\"GreenHat\",\n",
    "    model=model,\n",
    "    instruction=\"You are the Green Hat. Focus on creativity, new ideas, and alternatives. How can we bypass constraints? What are some wild or out-of-the-box solutions?\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a5734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Parallel Group\n",
    "# This runs all 5 agents at the same time on the user prompt\n",
    "thinking_team = ParallelAgent(\n",
    "    name=\"SixHatsBrainstorm\",\n",
    "    sub_agents=[white_hat, red_hat, black_hat, yellow_hat, green_hat]\n",
    ")\n",
    "\n",
    "# 4. Define the Blue Hat (The Manager)\n",
    "# This agent sees the combined output of the team and makes the plan\n",
    "blue_hat = Agent(\n",
    "    name=\"BlueHat\",\n",
    "    model=model,\n",
    "    instruction=\"\"\"\n",
    "    You are the Blue Hat. You are the manager of the thinking process. \n",
    "    You will receive input from 5 other agents (White, Red, Black, Yellow, Green).\n",
    "    Your job is to:\n",
    "    1. Synthesize their disparate points of view.\n",
    "    2. Resolve conflicts between the Black (Risk) and Yellow (Optimism) hats.\n",
    "    3. Produce a final, well-rounded decision or action plan.\n",
    "    \"\"\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Final Workflow\n",
    "# First run the team (Parallel), then run the manager (Sequential)\n",
    "solver_workflow = SequentialAgent(\n",
    "    name=\"SixHatsSolver\",\n",
    "    sub_agents=[thinking_team, blue_hat]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- EXECUTION ---\n",
    "# user_input = \"Should we switch our backend database from PostgreSQL to a NoSQL solution for our startup?\"\n",
    "# result = solver_workflow.run(user_input)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6ed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.runners import InMemoryRunner\n",
    "from google.adk.plugins.logging_plugin import (\n",
    "    LoggingPlugin,\n",
    ")  # <---- 1. Import the Plugin\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "\n",
    "runner = InMemoryRunner(\n",
    "    agent=solver_workflow,\n",
    "    plugins=[\n",
    "        LoggingPlugin()\n",
    "    ],  # <---- 2. Add the plugin. Handles standard Observability logging across ALL agents\n",
    ")\n",
    "print(\"âœ… Runner configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06d4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš€ Running agent with LoggingPlugin...\")\n",
    "print(\"ðŸ“Š Watch the comprehensive logging output below:\\n\")\n",
    "\n",
    "response = await runner.run_debug(\"Should we switch our backend database from PostgreSQL to a NoSQL solution for our startup?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
